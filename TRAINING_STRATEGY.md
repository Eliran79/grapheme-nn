# GRAPHEME Local Training Strategy

## Quick Start Guide

This document provides a step-by-step guide for training GRAPHEME on your local machine.

---

## Prerequisites

### Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| CPU | 4 cores | 8+ cores (Ryzen 7 / i7) |
| RAM | 16 GB | 32 GB |
| Disk | 50 GB SSD | 100 GB NVMe |
| GPU | Not required | Optional (future CUDA support) |

**Note**: GRAPHEME is CPU-optimized by design. The Rust implementation is 3 million times more efficient than transformer self-attention, so GPU is not critical initially.

### Software Requirements

```bash
# Rust toolchain (1.70+)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Verify installation
rustc --version
cargo --version
```

---

## Phase 1: Data Generation (CPU-Intensive)

### Understanding the Curriculum

GRAPHEME uses **curriculum learning** with 7 progressively harder levels:

| Level | Skill | Example | Samples |
|-------|-------|---------|---------|
| 1 | Basic arithmetic | `(+ 2 3)` → `5` | 10,000 |
| 2 | Nested operations | `(+ (* 2 3) 4)` → `10` | 50,000 |
| 3 | Symbol substitution | `(+ x 3)` [x=2] → `5` | 50,000 |
| 4 | Functions | `(sin 0)` → `0` | 100,000 |
| 5 | Differentiation | `(derive (^ x 2) x)` → `(* 2 x)` | 100,000 |
| 6 | Integration | `(integrate (^ x 2) x 0 1)` → `0.333` | 100,000 |
| 7 | Equation solving | `(solve (= (+ (* 2 x) 5) 13) x)` → `4` | 100,000 |

**Total**: ~510,000 verified training pairs

### Generate Training Data

```bash
# Create data directory
mkdir -p data/generated

# Generate all levels (parallel, uses all CPU cores)
cargo run --release -p grapheme-train --bin generate -- \
    --all-levels \
    --output data/generated/ \
    --format jsonl

# Or generate specific levels
cargo run --release -p grapheme-train --bin generate -- \
    --level 1 --samples 10000 \
    --output data/generated/level_1/

# Validate generated data
cargo run --release -p grapheme-train --bin validate -- \
    --input data/generated/
```

**Estimated Time** (8-core CPU):
- Level 1-2: ~5 minutes
- Level 3-4: ~20 minutes
- Level 5-7: ~1 hour
- Total: ~1.5 hours

### Data Format (JSONL)

Each line is a self-contained training example:

```json
{
  "id": "L2-00001",
  "level": 2,
  "input_polish": "(+ (* 2 3) 4)",
  "input_expr": {"BinOp": {"op": "Add", "left": {...}, "right": {...}}},
  "expected_result": 10.0,
  "expected_symbolic": null,
  "bindings": []
}
```

---

## Phase 2: Training Configuration

### Create Training Config

Create `train_config.toml`:

```toml
# train_config.toml

[training]
# Batch size (adjust based on RAM)
batch_size = 64

# Number of epochs per level
epochs_per_level = 10

# Learning rate
learning_rate = 0.001

# Early stopping patience
patience = 5

# Checkpoint frequency (epochs)
checkpoint_every = 2

[optimizer]
# Options: "sgd", "adam", "adamw"
type = "adam"

# Adam-specific
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

# Weight decay (L2 regularization)
weight_decay = 0.0001

[loss]
# Graph Edit Distance weights
node_insertion_cost = 1.0
node_deletion_cost = 1.0
edge_insertion_cost = 0.5
edge_deletion_cost = 0.5

# Clique mismatch penalty
clique_weight = 2.0

[curriculum]
# Start level (1-7)
start_level = 1

# End level (1-7)
end_level = 7

# Accuracy threshold to advance
advance_threshold = 0.95

# Minimum epochs before advancing
min_epochs_per_level = 5

[paths]
# Training data
train_data = "data/generated"

# Output directory
output_dir = "checkpoints"

# Log file
log_file = "training.log"

[hardware]
# Number of threads (0 = auto-detect)
num_threads = 0

# Enable parallel batch processing
parallel_batches = true
```

---

## Phase 3: Training Execution

### Start Training

```bash
# Build in release mode (critical for performance)
cargo build --release

# Run training with config
cargo run --release -p grapheme-train --bin train -- \
    --config train_config.toml \
    --verbose

# Or with inline options
cargo run --release -p grapheme-train --bin train -- \
    --data data/generated/ \
    --output checkpoints/ \
    --batch-size 64 \
    --epochs 10 \
    --lr 0.001
```

### Monitor Training

Training outputs metrics in real-time:

```
[Level 1] Epoch 1/10
  Batch 100/156: loss=0.234, GED=0.45
  Batch 156/156: loss=0.189, GED=0.38
  Epoch complete: avg_loss=0.201, accuracy=0.87

[Level 1] Epoch 2/10
  ...
  Epoch complete: avg_loss=0.098, accuracy=0.94

[Level 1] Advancing to Level 2 (accuracy 0.96 > 0.95)
```

### Log Analysis

```bash
# View training progress
tail -f training.log

# Plot metrics (requires Python + matplotlib)
python scripts/plot_training.py training.log
```

---

## Phase 4: Validation & Testing

### Validate Model

```bash
# Test on held-out validation set
cargo run --release -p grapheme-train --bin validate -- \
    --model checkpoints/latest.bin \
    --data data/generated/val/

# Test on edge cases
cargo run --release -p grapheme-train --bin validate -- \
    --model checkpoints/latest.bin \
    --data data/edge_cases/test_hard.jsonl
```

### Expected Results by Level

| Level | Target Accuracy | Acceptable |
|-------|----------------|------------|
| 1 | 99%+ | 95%+ |
| 2 | 98%+ | 92%+ |
| 3 | 95%+ | 88%+ |
| 4 | 92%+ | 85%+ |
| 5 | 85%+ | 75%+ |
| 6 | 80%+ | 70%+ |
| 7 | 75%+ | 65%+ |

---

## Phase 5: Optimization Tips

### Performance Tuning

```bash
# Enable maximum optimizations
RUSTFLAGS="-C target-cpu=native" cargo build --release

# Use all available parallelism
cargo run --release -- --threads 0  # auto-detect
```

### Memory Optimization

For machines with limited RAM:

```toml
[training]
batch_size = 32  # Reduce from 64

[hardware]
# Process one level at a time to reduce memory
stream_data = true
```

### Checkpointing Strategy

```bash
# Resume from checkpoint
cargo run --release -p grapheme-train --bin train -- \
    --resume checkpoints/epoch_5_level_2.bin \
    --config train_config.toml
```

---

## Training Pipeline Summary

```
┌─────────────────────────────────────────────────────────────┐
│                    GRAPHEME Training Pipeline                │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  1. DATA GENERATION                                          │
│     ┌─────────────┐                                          │
│     │   Engine    │──generates──▶ Verified Training Pairs    │
│     │  (Layer 1)  │              (infinite, correct)         │
│     └─────────────┘                                          │
│                                                              │
│  2. CURRICULUM LEARNING                                      │
│     Level 1 ──▶ Level 2 ──▶ ... ──▶ Level 7                  │
│     (basic)     (nested)           (equations)               │
│                                                              │
│  3. LOSS COMPUTATION                                         │
│     ┌─────────────────────────────────────────┐             │
│     │ Loss = α·node_ins + β·edge_del + γ·clique │            │
│     │       (Graph Edit Distance, NOT cross-entropy)        │
│     └─────────────────────────────────────────┘             │
│                                                              │
│  4. VALIDATION                                               │
│     Brain output ──verified by──▶ Engine                     │
│     (all outputs checked against ground truth)               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| Out of memory | Reduce `batch_size` to 32 or 16 |
| Slow training | Ensure `--release` mode; check CPU throttling |
| Loss not decreasing | Reduce `learning_rate` by 10x |
| Stuck at level | Increase `epochs_per_level`; lower `advance_threshold` |
| Data generation errors | Check `grapheme-engine` tests pass |

### Debug Mode

```bash
# Enable verbose logging
RUST_LOG=debug cargo run --release -- --verbose

# Run with backtrace on panic
RUST_BACKTRACE=1 cargo run --release -- --config train_config.toml
```

---

## Next Steps After Training

1. **Export Model**: Save trained weights for inference
2. **Benchmark**: Compare against external datasets (GSM8K, MATH)
3. **Fine-tune**: Add domain-specific training (NL augmentation)
4. **Deploy**: Integrate into applications

---

## Resource Estimates

### Disk Space

| Dataset | Size |
|---------|------|
| Level 1-2 | ~200 MB |
| Level 3-4 | ~1 GB |
| Level 5-7 | ~3 GB |
| Checkpoints | ~500 MB per level |
| **Total** | ~10 GB |

### Training Time (8-core CPU, 32GB RAM)

| Level | Time |
|-------|------|
| Level 1 | ~10 min |
| Level 2 | ~30 min |
| Level 3 | ~45 min |
| Level 4 | ~1 hour |
| Level 5-7 | ~2 hours each |
| **Total** | ~8-10 hours |

---

## Quick Reference Commands

```bash
# Full training pipeline
mkdir -p data/generated checkpoints

# Step 1: Generate data
cargo run --release -p grapheme-train --bin generate -- --all-levels --output data/generated/

# Step 2: Train
cargo run --release -p grapheme-train --bin train -- --config train_config.toml

# Step 3: Validate
cargo run --release -p grapheme-train --bin validate -- --model checkpoints/final.bin --data data/generated/test/

# Step 4: Interactive testing
cargo run --release -p grapheme-train --bin repl -- --model checkpoints/final.bin
```

---

*Self-generating, verified, curriculum-based training for true mathematical understanding.*
