---
id: api-009
title: Define Grounding Layer Traits (Phase 5)
status: done
priority: low
tags:
- api
- cognitive
- grounding
dependencies:
- api-006
assignee: developer
created: 2025-12-05T22:07:01.687584302Z
estimate: ~
complexity: 5
area: api
---

# Define Grounding Layer Traits (Phase 5)

## Context
GRAPHEME processes symbols but has no connection to meaning. "Cat" as a graph structure means nothing without connection to visual/tactile/behavioral representations of cats.

**Gap Analysis**: Without grounding, GRAPHEME:
- Manipulates symbols without understanding
- Cannot connect language to perception
- Has no embodied knowledge
- Cannot verify claims against reality

**Research Status**: Grounding remains an open research problem. Current LLMs arguably have weak grounding through massive co-occurrence statistics. This task defines the interface; implementation is exploratory.

## Objectives
- Define traits for symbol-referent binding
- Enable connection to perceptual representations
- Support embodied simulation
- Create interface for world interaction

## Tasks
- [ ] Define `Referent` and `WorldModel` types
- [ ] Define `GroundedGraph` trait
- [ ] Define perceptual binding interface
- [ ] Define action-perception loop
- [ ] Create `grapheme-ground` crate skeleton

## Acceptance Criteria
✅ **Symbol Grounding:**
- Symbols can bind to referents
- Binding is verifiable
- Multiple modality grounding

✅ **Embodiment:**
- Can simulate action consequences
- Perception updates internal model
- Action-perception loop works

## Technical Notes

### Core Structures
```rust
/// Something in the world that a symbol refers to
pub enum Referent {
    /// Perceptual representation (image region, sound, etc.)
    Perceptual(ModalGraph),

    /// Abstract concept (defined by relations to other concepts)
    Conceptual(Graph),

    /// Action/procedure
    Procedural(TransformRule),

    /// External entity (API, database, etc.)
    External(ExternalRef),
}

/// Grounding binding between symbol and referent
pub struct Grounding {
    pub symbol: NodeId,
    pub referent: Referent,
    pub confidence: f32,
    pub source: GroundingSource,
}

pub enum GroundingSource {
    Supervised,      // Explicitly labeled
    Inferred,        // Derived from context
    Embodied,        // Learned through interaction
    Linguistic,      // Co-occurrence statistics
}

/// World interaction interface
pub struct WorldInterface {
    pub sensors: Vec<Box<dyn Sensor>>,
    pub actuators: Vec<Box<dyn Actuator>>,
}
```

### Trait Definitions
```rust
/// Grounded graph with symbol-referent bindings
pub trait GroundedGraph: Send + Sync {
    /// Connect graph node to perceptual representation
    fn ground_to_perception(&mut self, node: NodeId, modality: Modality) -> Option<Grounding>;

    /// Bind symbol to external referent
    fn bind_referent(&mut self, node: NodeId, referent: Referent) -> Grounding;

    /// Get all groundings for a symbol
    fn groundings(&self, node: NodeId) -> Vec<Grounding>;

    /// Verify grounding against perception
    fn verify_grounding(&self, grounding: &Grounding, perception: &ModalGraph) -> f32;

    /// Simulate consequence of action
    fn simulate_consequence(&self, action: &Graph) -> Graph;
}

/// Sensor interface for perception
pub trait Sensor: Send + Sync {
    fn modality(&self) -> Modality;
    fn perceive(&mut self) -> ModalGraph;
    fn attend(&mut self, region: &Graph);
}

/// Actuator interface for action
pub trait Actuator: Send + Sync {
    fn execute(&mut self, action: &Graph) -> Result<(), ActionError>;
    fn can_execute(&self, action: &Graph) -> bool;
}

/// Embodied agent with perception-action loop
pub trait EmbodiedAgent: Send + Sync {
    /// Full perception-action cycle
    fn sense_think_act(&mut self, world: &mut WorldInterface) -> Graph;

    /// Update internal model from perception
    fn update_from_perception(&mut self, perception: &ModalGraph);

    /// Ground new symbols through interaction
    fn learn_grounding(&mut self, symbol: NodeId, interactions: &[Interaction]) -> Grounding;
}
```

### The Symbol Grounding Problem
```
┌─────────────┐     ┌─────────────┐
│   Symbol    │     │  Referent   │
│   "cat"     │────▶│  [visual]   │
│   (graph)   │     │  [tactile]  │
│             │     │  [behavior] │
└─────────────┘     └─────────────┘
      │                    │
      └────────┬───────────┘
               │
        How does the
        binding happen?

Options:
1. Supervised labeling (weak)
2. Co-occurrence statistics (LLM-style)
3. Embodied interaction (ideal but hard)
4. Simulation (compromise)
```

### Files to Create
- `grapheme-ground/Cargo.toml`
- `grapheme-ground/src/lib.rs`
- `grapheme-ground/src/grounding.rs`
- `grapheme-ground/src/perception.rs`
- `grapheme-ground/src/embodiment.rs`

## Testing
- [ ] Symbol-referent binding works
- [ ] Grounding verification produces scores
- [ ] Simulation predicts consequences
- [ ] Perception updates model

## Research Questions
- How to acquire grounding without embodiment?
- Can simulation substitute for real interaction?
- What's the minimum grounding for understanding?

## Updates
- 2025-12-05: Task created from AGI gap analysis

## Session Handoff (AI: Complete this when marking task done)
**For the next session/agent working on dependent tasks:**

### Dependencies & Integration
- Depends on: api-006 (MultiModal for perception)
- Top of grounding chain (no dependents yet)

### Open Problems
- True grounding may require embodiment
- Simulation fidelity vs. computational cost
- Grounding for abstract concepts