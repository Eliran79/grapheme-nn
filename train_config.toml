# train_config.toml - GRAPHEME Training Configuration

[training]
batch_size = 64
epochs_per_level = 10
learning_rate = 0.001
patience = 5
checkpoint_every = 2

[optimizer]
type = "adam"
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
weight_decay = 0.0001

[loss]
node_insertion_cost = 1.0
node_deletion_cost = 1.0
edge_insertion_cost = 0.5
edge_deletion_cost = 0.5
clique_weight = 2.0

[curriculum]
start_level = 1
end_level = 7
advance_threshold = 0.95
min_epochs_per_level = 5

[paths]
train_data = "data/generated"
output_dir = "checkpoints"
log_file = "training.log"

[hardware]
num_threads = 0
parallel_batches = true
